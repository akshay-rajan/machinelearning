{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Ensemble learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"weak learners\") are trained to solve the same problem and combined to get better results. The main hypothesis behind ensemble learning is that when weak models are correctly combined we can obtain more accurate and/or robust models.\n",
    "\n",
    "The main principle of ensemble learning is to generate multiple weak learners and combine their predictions in some way to make the final prediction. These weak learners can be generated by using different types of algorithms or the same algorithm with different parameters.\n",
    "\n",
    "There are several methods to combine these weak learners, such as:\n",
    "\n",
    "1. **Bagging**: Bagging tries to implement similar learners on small sample populations and then takes a mean of all the predictions. **Random forest** is a common algorithm that uses bagging.\n",
    "\n",
    "2. **Boosting**: Boosting is an iterative technique which adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Gradient Boosting and AdaBoost are examples of boosting algorithms. e.g. Gradient boosting, Adaptive boosting, XG boost etc.\n",
    "\n",
    "3. **Stacking**: Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.\n",
    "\n",
    "Ensemble methods can help to improve machine learning results by combining several models. This can often help to compensate for the weaknesses of a single model. Ensemble methods are often used in machine learning competitions (like `Kaggle`) to boost the performance of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging / Bootstrap Aggregating aims to `reduce the variance` of an ML model.\n",
    "\n",
    "1. Divide the dataset using a process called *bootstraping* (randomly sampling the dataset with replacement - the same instance can be sampled multiple times).\n",
    "2. A separate model is trained for each of these subsets.\n",
    "3. The final prediction is made by averaging the predictions (regression) or taking a majority vote (classification)\n",
    "\n",
    "Here, we are doing row sampling - dividing the dataset into subsets.\n",
    "\n",
    "Bagging is suited for underfitted (low bias, high variance) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique that is used to reduce bias as well as variance in supervised learning. It works on the principle of learners (weak learners), where it combines multiple weak learners to form a strong learner.\n",
    "\n",
    "Boosting involves training the weak learners *sequentially*.\n",
    "\n",
    "#### Each subsequent model is built by correcting the errors of its predecessor.\n",
    "\n",
    "The final prediction is typically a weighted sum of the predictions of each model.\n",
    "\n",
    "There are several types of boosting algorithms:\n",
    "\n",
    "1. AdaBoost (Adaptive Boosting): The first practical boosting algorithm, AdaBoost works by assigning weights to the instances, which are adjusted at each iteration to focus the next model on the hardest examples.\n",
    "\n",
    "2. Gradient Boosting: This algorithm uses the gradient of the loss function to guide the formation of the new model. This can result in better performance, but also requires careful tuning of the learning rate and other parameters.\n",
    "\n",
    "3. XGBoost (Extreme Gradient Boosting): An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework.\n",
    "\n",
    "Boosting is used to `reduce bias`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Ensemble learning is a machine learning paradigm where multiple models (often called \"base learners\" or \"weak learners\") are trained to solve the same problem and combined to get better results. The main hypothesis behind ensemble learning is that when weak models are correctly combined we can obtain more accurate and/or robust models.\n",
    "\n",
    "The main principle of ensemble learning is to generate multiple weak learners and combine their predictions in some way to make the final prediction. These weak learners can be generated by using different types of algorithms or the same algorithm with different parameters.\n",
    "\n",
    "There are several methods to combine these weak learners, such as:\n",
    "\n",
    "1. **Bagging**: Bagging tries to implement similar learners on small sample populations and then takes a mean of all the predictions. **Random forest** is a common algorithm that uses bagging.\n",
    "\n",
    "2. **Boosting**: Boosting is an iterative technique which adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Gradient Boosting and AdaBoost are examples of boosting algorithms. e.g. Gradient boosting, Adaptive boosting, XG boost etc.\n",
    "\n",
    "3. **Stacking**: Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.\n",
    "\n",
    "Ensemble methods can help to improve machine learning results by combining several models. This can often help to compensate for the weaknesses of a single model. Ensemble methods are often used in machine learning competitions (like `Kaggle`) to boost the performance of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging / Bootstrap Aggregating aims to reduce the variance of an ML model.\n",
    "\n",
    "1. Divide the dataset using a process called *bootstraping* (randomly sampling the dataset with replacement - the same instance can be sampled multiple times).\n",
    "2. A separate model is trained for each of these subsets.\n",
    "3. The final prediction is made by averaging the predictions (regression) or taking a majority vote (classification)\n",
    "\n",
    "Here, we are doing row sampling - dividing the dataset into subsets."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

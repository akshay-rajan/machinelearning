{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a classification algorithm (supervised learning algorithm for discrete data).\n",
    "\n",
    "It is used for `binary classification` problems. Logistic regression predicts the probability of an event occuring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hours studied \n",
    "X = np.array([0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5]).reshape(-1, 1)\n",
    "\n",
    "# Pass (1) or fail (0)\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  Fail\n",
      "Prediction:  Pass\n"
     ]
    }
   ],
   "source": [
    "# Predict pass or fail\n",
    "new_X = [[2]] # 2 hours studied\n",
    "prediction = model.predict(new_X)\n",
    "\n",
    "print(\"Prediction: \", \"Pass\" if prediction[0] == 1 else \"Fail\")\n",
    "\n",
    "new_X = [[3]] # 3 hours studied\n",
    "prediction = model.predict(new_X)\n",
    "\n",
    "print(\"Prediction: \", \"Pass\" if prediction[0] == 1 else \"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic Regression uses a `sigmoid function`, to squeeze the output of a linear equation between 0 and 1.\n",
    "\n",
    "```\n",
    "sigmoid(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "The cost function,\n",
    "\n",
    "`Cost(h(x), y) = -y * log(h(x)) - (1 - y) * log(1 - h(x))` where `h(x)` is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predict_1 = y * np.log(sigmoid(z))\n",
    "    predict_0 = (1 - y) * np.log(1 - sigmoid(z))\n",
    "    return -sum(predict_1 + predict_0) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.\n",
    "\n",
    "\n",
    "1. Initialize the parameters of the model with some values.\n",
    "2. Calculate the gradient of the cost function with respect to each parameter. The gradient is a vector that points in the direction of the greatest rate of increase of the function, and its magnitude is the rate of increase in that direction.\n",
    "3. Update the parameters by moving in the direction of the negative gradient (i.e., the direction of steepest descent). The size of the step is determined by the learning rate, a hyperparameter that you choose.\n",
    "4. Repeat steps 2 and 3 until the algorithm converges to a minimum.\n",
    "\n",
    "There are several variants of gradient descent that differ in how much data we use to compute the gradient of the objective function. The three main forms are batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.\n",
    "\n",
    "- **Batch gradient descent** computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. However, it is terribly inefficient for large datasets.\n",
    "\n",
    "- **Stochastic gradient descent (SGD)** computes the gradient using a single sample. SGD can be faster than batch gradient descent, since it performs updates more frequently. It can also introduce noise into the gradient descent process, which can help avoid local minima.\n",
    "\n",
    "- **Mini-batch gradient descent** is a compromise between batch gradient descent and SGD. It uses a mini-batch of `n` samples to compute the gradient at each step. This can be more efficient than SGD for many machine learning problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, weights, alpha, num_iterations):\n",
    "    m = len(y)\n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, weights)\n",
    "        h = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "        weights -= alpha * gradient\n",
    "        cost = cost_function(X, y, weights)\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.690616095200909\n",
      "Cost after iteration 1000: 0.518972736605199\n",
      "Cost after iteration 2000: 0.46474846732535574\n",
      "Cost after iteration 3000: 0.4386448374933137\n",
      "Cost after iteration 4000: 0.42463199409812324\n",
      "Cost after iteration 5000: 0.4165035663973164\n",
      "Cost after iteration 6000: 0.4115206358923655\n",
      "Cost after iteration 7000: 0.40833995868307565\n",
      "Cost after iteration 8000: 0.40624707399606014\n",
      "Cost after iteration 9000: 0.40483741299062537\n",
      "Weights after gradient descent:  [-3.55491702  1.32858743]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5]).reshape(-1, 1)\n",
    "# Add a 1 to each input to account for the bias term\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros(X.shape[1])\n",
    "\n",
    "alpha = 0.01 # learning rate\n",
    "num_iterations = 10000\n",
    "\n",
    "# Run gradient descent\n",
    "weights = gradient_descent(X, y, weights, alpha, num_iterations)\n",
    "\n",
    "print(\"Weights after gradient descent: \", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization is a solution to overfitting, in which we remove some of the features. The types of regularization are\n",
    "\n",
    "1. L1 Regularization\n",
    "2. L2 Regularization\n",
    "3. Elastic Net Regularization\n",
    "\n",
    "L1 Regularization adds a penalty term to the cost function that is proportional to the absolute value of the coefficients. This encourages sparsity in the model, meaning it will set some coefficients to zero, effectively removing those features from the model.\n",
    "\n",
    "L2 Regularization adds a penalty term to the cost function that is proportional to the square of the coefficients. This encourages smaller coefficients, effectively shrinking them towards zero.\n",
    "\n",
    "Elastic Net Regularization is a combination of L1 and L2 regularization. It adds both penalty terms to the cost function, allowing for a balance between sparsity and shrinkage.\n",
    "\n",
    "Regularization helps to prevent overfitting by reducing the complexity of the model and improving its generalization ability. It is particularly useful when dealing with high-dimensional datasets or when there is a large number of features compared to the number of observations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a powerful ensemble machine learning algorithm that's used for both regression and classification problems. It works by combining multiple weak learners to create a strong learner. The weak learners are typically decision trees.\n",
    "\n",
    "1. Initialization: The algorithm starts by **predicting a constant value** for all instances in the dataset. This could be, for example, the mean of the target values for a regression problem.\n",
    "\n",
    "2. Iteration: The algorithm then enters a loop, which is repeated for a specified number of iterations, or until the error reaches an acceptable level. In each iteration:\n",
    "\n",
    "    * The residuals (differences between the predicted and actual values) are calculated.\n",
    "    * A new model is fit to the residuals.\n",
    "    * The predictions from this model are scaled using a learning rate (also called shrinkage or step size), and added to the existing predictions.\n",
    "\n",
    "3. Final Model: The final model is a weighted sum of the predictions from the individual models.\n",
    "\n",
    "The key idea behind Gradient Boosting is to set the target outcomes for this next model in order to minimize the error. The target outcome for each case in the dataset is set to be the residuals from the prediction of the first model. Models are added sequentially until no further improvements can be made.\n",
    "\n",
    "A popular implementation of Gradient Boosting is XGBoost, which stands for Extreme Gradient Boosting. XGBoost provides a parallel tree boosting that solve many data science problems in a fast and accurate way. It has gained much popularity and attention recently as it was the algorithm of choice for many winning teams of a number of machine learning competitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 27.145433310151915\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)\n",
    "\n",
    "# Train the model using the training sets\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", sqrt(mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

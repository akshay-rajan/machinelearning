{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Algorithm\n",
    "\n",
    "Gradient Boosting is a powerful ensemble machine learning algorithm that's used for both regression and classification problems. It works by combining multiple weak learners to create a strong learner. The weak learners are typically decision trees.\n",
    "\n",
    "1. Initialization: The algorithm starts by **predicting a constant value** for all instances in the dataset. This could be, for example, the mean of the target values for a regression problem.\n",
    "\n",
    "2. Iteration: The algorithm then enters a loop, which is repeated for a specified number of iterations, or until the error reaches an acceptable level. In each iteration:\n",
    "\n",
    "    * The residuals (differences between the predicted and actual values) are calculated.\n",
    "    * A new model is fit to the residuals.\n",
    "    * The predictions from this model are scaled using a learning rate (also called shrinkage or step size), and added to the existing predictions.\n",
    "\n",
    "3. Final Model: The final model is a weighted sum of the predictions from the individual models.\n",
    "\n",
    "The key idea behind Gradient Boosting is to set the target outcomes for this next model in order to minimize the error. The target outcome for each case in the dataset is set to be the residuals from the prediction of the first model. Models are added sequentially until no further improvements can be made.\n",
    "\n",
    "A popular implementation of Gradient Boosting is XGBoost, which stands for Extreme Gradient Boosting. XGBoost provides a parallel tree boosting that solve many data science problems in a fast and accurate way. It has gained much popularity and attention recently as it was the algorithm of choice for many winning teams of a number of machine learning competitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Classifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)\n",
    "\n",
    "# Train the model using the training sets\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 27.145433310151915\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=4, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)\n",
    "\n",
    "# Train the model using the training sets\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"RMSE:\", sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "AdaBoost, short for Adaptive Boosting, is a machine learning algorithm that is used as an ensemble method to improve the model. It's a type of \"Boosting\" algorithm which aims to convert a set of weak classifiers into a strong one.\n",
    "\n",
    "1. **Initialization**: Each data point in the training set is assigned an equal weight.\n",
    "\n",
    "2. **Training**: A weak classifier (often a decision stump, which is a one-level decision tree) is trained on the data.\n",
    "\n",
    "3. **Weight Update**: After training, the weights of the data points are updated. The weights of the incorrectly classified points are increased, and the weights of the correctly classified points are decreased. This means that the algorithm pays more attention to the points it got wrong in the next round of training.\n",
    "\n",
    "4. **Iteration**: Steps 2 and 3 are repeated for a specified number of iterations, or until the error reaches an acceptable level. In each iteration, a new weak classifier is trained on the data, taking into account the updated weights.\n",
    "\n",
    "5. **Final Model**: The final model is a weighted combination of the weak classifiers. The weight of each classifier in the final model is proportional to its accuracy, with more accurate classifiers given more weight.\n",
    "\n",
    "AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers but is less susceptible to the overfitting problem than most learning algorithms.\n",
    "\n",
    "Every learning algorithm tends to suit some problem types better than others, and typically has many different parameters and configurations to adjust before it achieves optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost, short for \"Extreme Gradient Boosting\", is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework.\n",
    "\n",
    "1. **Initialization**: XGBoost starts with an initial prediction, which can be the mean of the target variable for a regression problem.\n",
    "\n",
    "2. **Tree Building**: For each tree, the features are split in a way that reduces the loss (difference between the actual and predicted values). The output for each leaf of the tree is a score that minimizes the loss when added to the previous trees' predictions.\n",
    "\n",
    "3. **Regularization**: XGBoost also introduces regularization parameters to penalize complex models, which helps to control overfitting.\n",
    "\n",
    "4. **Weighted Quantile Sketch**: XGBoost employs the Weighted Quantile Sketch algorithm to effectively find the optimal split points when handling continuous features.\n",
    "\n",
    "5. **Cross-validation**: The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.\n",
    "\n",
    "XGBoost has gained much popularity and attention recently as it was the algorithm of choice for many winning teams of a number of machine learning competitions. It's known for its speed and performance, as it includes a parallel processing feature that makes it faster than other gradient boosting techniques. It also includes a variety of regularization techniques that reduce overfitting and improve overall performance. It is robust enough to support fine-tuning and addition of custom regularization parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

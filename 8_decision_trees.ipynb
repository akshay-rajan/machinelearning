{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Tree Algorithm is a supervised learning algorithm used for both classification as well as regression.\n",
    "\n",
    "A **Decision Tree** is just a collections nested if-else statements.\n",
    "\n",
    "![decisiontree](./files/ml_decision_tree.png)\n",
    "\n",
    "The decision tree builds models in the form of a tree structure, breaking down a dataset into smaller and smaller subsets while developing an associated decision tree incrementally. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "![decisionTree-iris](./files/Decision-tree-for-Iris-dataset.png)\n",
    "\n",
    "    The first node is the `root`/parent node and all other nodes are child nodes.\n",
    "\n",
    "The `terminal node / leaf node` has no children - they are not split any further. \n",
    "\n",
    "A subtree of a decision tree is called a `branch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision Nodes: These are the nodes where a feature is tested, and the data is split based on the result of the test. The feature and the value to test are chosen to maximize the separation of the classes in the data. The specific measure used to determine the best split varies, but common ones include Gini impurity and information gain.\n",
    "\n",
    "2. Leaf Nodes: These are the nodes where a prediction is made. Once the data reaches a leaf node, no further splits are made. In a classification tree, the prediction is the most common class in the data at the leaf node. In a regression tree, the prediction is typically the mean target value of the data at the leaf node.\n",
    "\n",
    "3. Tree Building: The tree is built in a top-down, recursive manner. Starting with all the data at the root, the best feature to split on is chosen. The data is then divided into subsets, which each become the child nodes. This process is repeated recursively on each child node, until a stopping condition is met (e.g., maximum depth is reached, no feature provides a reduction in impurity).\n",
    "\n",
    "4. `Pruning`: To avoid overfitting, the tree is often pruned after it's built. This involves removing sections of the tree that provide little power to classify instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the metrics used to split the data in decision trees include\n",
    "\n",
    "1. **Entropy**: Measure of randomness. The measure of the amount of uncertainity associated with the outcomes of a node is the entropy of that node. The goal of the decision tree is to reduce the entropy of each node. A leaf node is a node with entropy 0.\n",
    "\n",
    "2. **Information Gain**: Reduction in entropy achieved by partitioning the data on an attribute. Information gain is the difference between the entropy of the parent node and the weighted sum of the entropy of the child nodes. The attribute with the highest information gain is chosen as the splitting attribute at each node.\n",
    "\n",
    "3. **Gini Impurity**: Another measure of impurity or disorder. Gini impurity is the probability of a random sample being classified incorrectly if you randomly pick a label according to the distribution in a branch. It's used instead of entropy to reduce the time of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "# model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Tree Algorithm is a supervised learning algorithm used for both classification as well as regression.\n",
    "\n",
    "A **Decision Tree** is just a collections nested if-else statements.\n",
    "\n",
    "![decisiontree](./files/ml_decision_tree.png)\n",
    "\n",
    "The decision tree builds models in the form of a tree structure, breaking down a dataset into smaller and smaller subsets while developing an associated decision tree incrementally. The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "![decisionTree-iris](./files/Decision-tree-for-Iris-dataset.png)\n",
    "\n",
    "    The first node is the `root`/parent node and all other nodes are child nodes.\n",
    "\n",
    "The `terminal node / leaf node` has no children - they are not split any further. \n",
    "\n",
    "A subtree of a decision tree is called a `branch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision Nodes: These are the nodes where a feature is tested, and the data is split based on the result of the test. The feature and the value to test are chosen to maximize the separation of the classes in the data. The specific measure used to determine the best split varies, but common ones include Gini impurity and information gain.\n",
    "\n",
    "2. Leaf Nodes: These are the nodes where a prediction is made. Once the data reaches a leaf node, no further splits are made. In a classification tree, the prediction is the most common class in the data at the leaf node. In a regression tree, the prediction is typically the mean target value of the data at the leaf node.\n",
    "\n",
    "3. Tree Building: The tree is built in a top-down, recursive manner. Starting with all the data at the root, the best feature to split on is chosen. The data is then divided into subsets, which each become the child nodes. This process is repeated recursively on each child node, until a stopping condition is met (e.g., maximum depth is reached, no feature provides a reduction in impurity).\n",
    "\n",
    "4. `Pruning`: To avoid overfitting, the tree is often pruned after it's built. This involves removing sections of the tree that provide little power to classify instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the metrics used to split the data in decision trees include\n",
    "\n",
    "1. **Entropy**: Measure of randomness. Measure of the amount of uncertainity associated with the outcomes of a node.\n",
    "\n",
    "2. **Information Gain**:\n",
    "\n",
    "3. **Gini Impurity**:\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance\n",
    "\n",
    "Bias and variance are two fundamental concepts in the field of machine learning that describe the tradeoff between a model's complexity and its accuracy in predicting outcomes on unseen data.\n",
    "\n",
    "1. **Bias**: Bias refers to the error introduced by approximating a real-world problem, which may be extremely complicated, by a much simpler model. For example, assuming data is linear when it actually has a more complicated structure. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting), meaning the model is too simple to capture all the information in the data.\n",
    "\n",
    "2. **Variance**: Variance, on the other hand, refers to the amount by which our model would change if we estimated it using a different training dataset. High variance can cause an algorithm to model the random noise in the training data (overfitting), meaning the model is too complex and is capturing the noise along with the underlying pattern in the data.\n",
    "\n",
    "In essence, bias is error from erroneous assumptions in the learning algorithm, leading to a less complex model. Variance is error from being overly sensitive to small fluctuations in the training data, leading to a more complex model.\n",
    "\n",
    "The goal in a machine learning model is to balance bias and variance, to avoid overfitting or underfitting. This trade-off is known as the Bias-Variance Tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting in machine learning are directly related to the concepts of bias and variance.\n",
    "\n",
    "1. **Underfitting**: This occurs when a model is too simple to capture the underlying structure of the data. It has **high bias and low variance**. The model makes a lot of assumptions about the data and as a result, it performs poorly on both the training data and unseen data. This is because it oversimplifies the problem and misses important relationships between variables.\n",
    "\n",
    "2. **Overfitting**: This occurs when a model is too complex and captures the noise along with the underlying pattern in data. It has **low bias and high variance**. The model performs well on the training data but poorly on unseen data. This is because it is too sensitive to the random noise in the training data and ends up learning from it, which then negatively impacts its performance on new data.\n",
    "\n",
    "In machine learning, we aim to find the right balance between bias and variance. Too much bias leads to underfitting, where the model is too simple to capture all the information in the data. Too much variance leads to overfitting, where the model is so complex that it starts learning from the noise in the data. The goal is to find a sweet spot where we minimize total error, which is the sum of bias and variance. This is known as the bias-variance tradeoff."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
